# Dataset formats

Each TRL trainer supports a specific dataset format. In this document, we provide an overview of the dataset formats supported by each trainer. Additionally, since conversational datasets are very common, we also provide a guide on how to use them, and how to convert them into a standard dataset format for TRL trainers.

## Overview of the dataset formats

<table>
  <tr>
    <th>Dataset type</th>
    <th>Standard</th>
    <th>Conversational</th>
  </tr>
  <tr>
    <td>Language Modeling</td>
    <td>
      <pre><code>{"text": "The sky is blue."}</code></pre>
    </td>
    <td>
      <pre><code>{"messages": [{"role": "user", "content": "What color is the sky?"},
              {"role": "assistant", "content": "It is blue."}]}</code></pre>
    </td>
  </tr>
  <tr>
    <td>Prompt-only</td>
    <td>
      <pre><code>{"prompt": "The sky is"}</code></pre>
    </td>
    <td>
      <pre><code>{"prompt": [{"role": "user", "content": "What color is the sky?"}]}</code></pre>
    </td>
  </tr>
  <tr>
    <td>Prompt-completion</td>
    <td>
      <pre><code>{"prompt": "The sky is"
 "completion": " blue."}</code></pre>
    </td>
    <td>
      <pre><code>{"prompt": [{"role": "user", "content": "What color is the sky?"}],
 "completion": [{"role": "assistant", "content": "It is blue."}]}</code></pre>
    </td>
  </tr>
  </tr>
  <tr>
    <td>Preference</td>
    <td>
      <pre><code>{"prompt": "The sky is",
 "chosen": " blue.",
 "rejected": " green."}</code></pre>
      or, with implicit prompt:
      <pre><code>{"chosen": "The sky is blue.",
 "rejected": "The sky is green."}</code></pre>
    </td>
    <td>
      <pre><code>{"prompt": [{"role": "user", "content": "What color is the sky?"}],
 "chosen": [{"role": "assistant", "content": "It is blue."}],
 "rejected": [{"role": "assistant", "content": "It is green."}]}</code></pre>
      or, with implicit prompt:
      <pre><code>{"chosen": [{"role": "user", "content": "What color is the sky?"},
              {"role": "assistant", "content": "It is blue."}],
  "rejected": [{"role": "user", "content": "What color is the sky?"},
                {"role": "assistant", "content": "It is green."}]}</code></pre>
    </td>
  </tr>
    <td>Unpaired preference</td>
    <td>
      <pre><code>{"prompt": "The sky is",
 "completion": " blue.",
 "label": True}</code></pre>
    </td>
    <td>
      <pre><code>{"prompt": [{"role": "user", "content": "What color is the sky?"}],
 "completion": [{"role": "assistant", "content": "It is green."}],
 "label": False}</code></pre>
    </td>
  </tr>
</table>

## Which dataset format to use?

TRL trainers only support standard datasets. Conversational datasets need to be formatted first to match the standard dataset format. To more info, see above

| Trainer    | Expected dataset format |
| ---------- | ----------------------- |
| SFT        | Prompt-completion       |
| DPO        | Preference              |
| KTO        | Unpaired preference     |
| Online-DPO | Prompt-only             |


## Conversational dataset

Conversational dataset are very common.

Important: TRL trainer  don't support conversational datasets as is. They need to be converted into a standard dataset format. But don't worry, be provide tools to easily convert them, see below.

### A brief introduction to conversational dataset

Usually, conversational datasets are formatted as a list of messages, where each message has a role (e.g., `"user"` or `"assistant"`) and a content (e.g., `"Hello, how are you?"`).
Example:


```python
messages = [
    {"role": "user", "content": "Hello, how are you?"},
    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
    {"role": "user", "content": "I'd like to show off how chat templating works!"},
]
```

You can find many datasets like that, for example:

<iframe
  src="https://huggingface.co/datasets/trl-lib/kto-mix-14k/embed/viewer/default/train?row=0"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>

For an in-depth guide on how to use chat datasets, see the [Chat templating `transformers`'s documentation](https://huggingface.co/docs/transformers/en/chat_templating).

### Convert a conversational dataset into a standard dataset

TRL trainers don't support conversational as is, but need to be converted into a standard dataset. For converting a conversational dataset into a standard dataset, we use a chat template. The way you want to apply the chat template to the dataset depends of the task you want to do. Hopefully, TRL provides an helper function to easily perform this conversion. it's the [`apply_chat_template`].  Here is an example of how to use it:

```python
>>> from transformers import AutoTokenizer
>>> from trl import apply_chat_template
>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct")
>>> example = {
...     "prompt": [{"role": "user", "content": "What color is the sky?"}],
...     "completion": [{"role": "assistant", "content": "It is blue."}]
... }
>>> apply_chat_template(example, tokenizer)
{'prompt': '<|user|>\nWhat color is the sky?<|end|>\n<|assistant|>\n', 'completion': 'It is blue.<|end|>\n<|endoftext|>'}
```

or, using the map method:

```python
>>> from datasets import Dataset
>>> dataset_dict = {
...     "prompt": [[{"role": "user", "content": "What color is the sky?"}],
...                [{"role": "user", "content": "Where is the sun?"}]],
...     "completion": [[{"role": "assistant", "content": "It is blue."}],
...                    [{"role": "assistant", "content": "In the sky."}]]}
dataset = Dataset.from_dict(dataset_dict)
dataset = dataset.map(maybe_apply_chat_template, fn_kwargs={"tokenizer": tokenizer})
{'prompt': ['<|user|>\nWhat color is the sky?<|end|>\n<|assistant|>\n', '<|user|>\nWhere is the sun?<|end|>\n<|assistant|>\n'],
 'completion': ['It is blue.<|end|>\n<|endoftext|>', 'In the sky.<|end|>\n<|endoftext|>']}
```

Every model has its own chat template.

Caution: it's very important to use the template associated with the model you're using. As an example, you can't use a dataset formated with the Qwen chat template to finetune a Llama model.

> [!CAUTION]
> It's important to use the chat template associated with the model you're using. Chat templates are model-specific, and using the wrong template can lead to unexpected results.

To convert a conversational dataset into a standard dataset, TRL provides a function called `convert_conversational_to_standard` that takes a conversational dataset and a tokenizer as input and returns a standard dataset.

Here is an example with prompt-only dataset, and the Qwen chat template:

```python
from datasets import Dataset
from transformers import AutoTokenizer

conversational_dataset_dict = {
    "prompt": [
        [{"role": "user", "content": "What color is the sky?"}],
    ]
}

conversational_dataset = Dataset.from_dict(conversational_dataset_dict)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-72B-Instruct")

standard_dataset = conversational_dataset.map(convert_conversational_to_standard, fn_kwargs={"tokenizer": tokenizer})
```

The resulting `standard_dataset` will look like this:

```python
>>> print(standard_dataset[0]["prompt"])
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What color is the sky?<|im_end|>
<|im_start|>assistant
```


> [!IMPORTANT]
> Make sure that the SFT model and reward model use the _same_ chat template. Otherwise, you may find the model completions are scored incorrectly during training.
> 

Note: once again, it's very important to use the template associated with the model you're using. As an example, here is what you get if you use the chat template of the Llama model instead:

```python
>>> print(standard_dataset[0]["prompt"])
<|user|>
What color is the sky?<|end|>
<|assistant|>
```


## Utilities for converting dataset formats

### From conversational to standard dataset

[[autodoc]] maybe_apply_chat_template

### From paired to unpaired preference dataset

[[autodoc]] maybe_unpair_preference_dataset

### From implicit prompt to explicit prompt dataset

[[autodoc]] maybe_extract_prompt

